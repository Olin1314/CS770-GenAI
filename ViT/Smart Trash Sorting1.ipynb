{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import profiler\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from model import ViT\n",
    "from trainer_utils import save_checkpoint, config_logger, updated_checkpoint\n",
    "from data_preparation import prepare_data_loaders, prepare_test_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for training the model\n",
    "\n",
    "    Args:\n",
    "    model (ViT): The model used for the experiment\n",
    "    optimizer (optim): Optimizer used for training\n",
    "    loss_func (nn): Loss function used for training\n",
    "    exp_name (str): Name of the experiment\n",
    "    device (str): Device to use for training\n",
    "    scheduler (optim.lr_scheduler): Scheduler used for training\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optimizer, loss_func, device, scheduler):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "\n",
    "    def train(self, trainloader, valloader, epochs, early_stop_patience = 5):\n",
    "        \"\"\"\n",
    "        Training function for the model\n",
    "\n",
    "        Args:\n",
    "        trainloader (DataLoader): DataLoader for training data\n",
    "        valloader (DataLoader): DataLoader for validation data\n",
    "        epochs (int): Number of epochs to train the model\n",
    "        early_stop_patience (int, optional): Number of epochs to wait for improvement in validation loss before early stopping. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        train_losses, val_losses, accuracies = [], [], []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        try:\n",
    "            outdir = os.path.join(\"recogition\", datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                start = time()\n",
    "                train_loss = self.train_epoch(trainloader)\n",
    "                val_accuracy, val_loss = self.test(valloader)\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                accuracies.append(val_accuracy)\n",
    "\n",
    "                logging.info(f\"Epoch: {epoch + 1}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.4f}, Time: {time() - start:.4f}\")\n",
    "\n",
    "                # Early stopping condition\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    save_checkpoint(self.model, epoch + 1, outdir)\n",
    "                    logging.info(\"-------- Saved Best Model! --------\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    logging.info(\"Early Stop Left: {}\".format(early_stop_patience - patience_counter))\n",
    "\n",
    "                if (early_stop_patience - patience_counter) == 0:\n",
    "                    logging.info(\"-------- Early Stop! --------\")\n",
    "                    break\n",
    "\n",
    "            save_checkpoint(self.model, epochs, outdir)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logging.info(\"Keyboard interrupt detected. Saving the model...\")\n",
    "            save_checkpoint(self.model, epoch + 1, outdir)\n",
    "            logging.info(\"Model saved successfully.\")\n",
    "\n",
    "        return train_losses, val_losses, accuracies\n",
    "\n",
    "\n",
    "    def train_epoch(self, trainloader):\n",
    "        \"\"\"\n",
    "        Training function for one epoch\n",
    "\n",
    "        Args:\n",
    "        trainloader (DataLoader): DataLoader for training data\n",
    "\n",
    "        Returns:\n",
    "        train_loss (float): Training loss\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in trainloader:\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            images, labels = batch\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * len(images)\n",
    "            self.scheduler.step(loss)\n",
    "        \n",
    "        return total_loss / len(trainloader.dataset)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, testloader):\n",
    "        \"\"\"\n",
    "        Testing function for the model\n",
    "\n",
    "        Args:\n",
    "        testloader (DataLoader): DataLoader for testing data\n",
    "\n",
    "        Returns:\n",
    "        accuracy (float): Accuracy of the model\n",
    "        avg_loss (float): Average loss of the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                logits = self.model(images)\n",
    "\n",
    "                loss = self.loss_func(logits, labels)\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, accuracies):\n",
    "    \"\"\"\n",
    "    Plot the training and validation metrics\n",
    "\n",
    "    Args:\n",
    "    train_losses (list): List of training losses\n",
    "    val_losses (list): List of validation losses\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracies, label='Validation Accuracy', color='green')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"recogition\", \"metrics.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, device, testloader, loss_func):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch in testloader:\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        images, labels = batch\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = loss_func(logits, labels)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(predictions == labels).item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    avg_loss = total_loss / len(testloader.dataset)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def test_visualize(model, device, testloader, classes):\n",
    "        \"\"\"\n",
    "        Visualize the predictions of the model\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                batch = [t.to(device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                logits = model(images)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "                for i in range(len(images)):\n",
    "                    image = images[i]\n",
    "                    label = labels[i]\n",
    "                    prediction = predictions[i]\n",
    "\n",
    "                    plt.imshow(image.permute(1, 2, 0).cpu())\n",
    "                    plt.title(f\"Label: {classes[label]}, Prediction: {classes[prediction]}\")\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "def setup_seed(seed=3456):\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\tnp.random.seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\n",
    "\ttorch.backends.cudnn.deterministics = True\n",
    "\ttorch.backends.cudnn.benchmarks = False\n",
    "\ttorch.backends.cudnn.enabled = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"./data/\"\n",
    "log_path = os.path.join(\"recogition\", \"train_\" + datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + \".log\")\n",
    "num_workers = 0\n",
    "\n",
    "batch_size = 1280\n",
    "epochs = 80\n",
    "learning_rate = 0.001\n",
    "patch_size = 16\n",
    "hidden_size = 64\n",
    "num_hidden_layers = 2\n",
    "num_attention_heads = 3\n",
    "intermediate_size = 4 * hidden_size\n",
    "hidden_dropout_prob = 0.041\n",
    "attention_probs_dropout_prob = 0.122\n",
    "image_size = 224\n",
    "num_channels = 3\n",
    "qkv_bias = True\n",
    "early_stop_patience = 5\n",
    "\n",
    "\n",
    "def main(continue_train=False, testing=False, test_data_dir='./data/test'):\n",
    "    if not testing:\n",
    "        config_logger(log_path)\n",
    "\n",
    "        logging.info(\"-------- Preparing Dataset! --------\")\n",
    "        trainloader, valloader, testloader, classes = prepare_data_loaders(data_dir, batch_size=batch_size, num_workers=num_workers)\n",
    "        logging.info(\"-------- Dataset prepared! --------\\n\\n\")\n",
    "\n",
    "\n",
    "        logging.info(\"-------- Creating Model! --------\")\n",
    "        model = ViT(image_size, hidden_size, num_hidden_layers, intermediate_size, len(classes), num_attention_heads, hidden_dropout_prob, \n",
    "                    attention_probs_dropout_prob, num_channels, patch_size, qkv_bias).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        trainer = Trainer(model, optimizer, loss_func, device, scheduler)\n",
    "        logging.info(\"-------- Model Created! --------\\n\\n\")\n",
    "\n",
    "        logging.info(\"-------- Training Model --------\")\n",
    "        if not continue_train:\n",
    "            train_losses, val_losses, accuracies = trainer.train(trainloader, valloader, epochs, early_stop_patience)\n",
    "        else:\n",
    "            model_path = updated_checkpoint()\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            logging.info(f\"-------- Load Model from {model_path}! --------\")\n",
    "\n",
    "            train_losses, val_losses, accuracies = trainer.train(trainloader, valloader, epochs, early_stop_patience)\n",
    "        logging.info(\"-------- Training Model Finished! --------\\n\\n\")\n",
    "\n",
    "\n",
    "        logging.info(\"-------- Start to Test! --------\")\n",
    "        accuracy, avg_loss = trainer.test(testloader)\n",
    "        logging.info(f\"Test loss: {avg_loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "        logging.info(\"-------- Testing Ended! --------\\n\\n\")\n",
    "\n",
    "\n",
    "        plot_metrics(train_losses, val_losses, accuracies)\n",
    "\n",
    "    else:\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        model_path = updated_checkpoint()\n",
    "        testloader, classes = prepare_test_data_loader(test_data_dir, batch_size=batch_size, num_workers=num_workers)\n",
    "        model = ViT(image_size, hidden_size, num_hidden_layers, intermediate_size, len(classes), num_attention_heads, hidden_dropout_prob, \n",
    "                    attention_probs_dropout_prob, num_channels, patch_size, qkv_bias)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        accuracy, avg_loss = test(model, device, testloader, loss_func)\n",
    "        print(f\"Test loss: {avg_loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    epochs = trial.suggest_int('epochs', 10, 250)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.001, 0.01)\n",
    "    patch_size = trial.suggest_categorical(\"patch_size\", [16, 32])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 48, 64])\n",
    "    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 2, 5)\n",
    "    num_attention_heads = trial.suggest_int(\"num_attention_heads\", 2, 5)\n",
    "    hidden_dropout_prob = trial.suggest_uniform(\"hidden_dropout_prob\", 0.0, 0.5)\n",
    "    attention_probs_dropout_prob = trial.suggest_uniform(\"attention_probs_dropout_prob\", 0.0, 0.5)\n",
    "\n",
    "    intermediate_size = 4 * hidden_size\n",
    "\n",
    "    logging.info(\"-------- Preparing Dataset! --------\")\n",
    "    trainloader, valloader, testloader, classes = prepare_data_loaders(data_dir, batch_size=batch_size, num_workers=num_workers)\n",
    "    logging.info(\"-------- Dataset Prepared --------\\n\\n\")\n",
    "\n",
    "    model = ViT(image_size, hidden_size, num_hidden_layers, intermediate_size, len(classes), num_attention_heads, hidden_dropout_prob, \n",
    "                    attention_probs_dropout_prob, num_channels, patch_size, qkv_bias).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, loss_func, device, scheduler)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time()\n",
    "        train_loss = trainer.train_epoch(trainloader)\n",
    "        end = time()\n",
    "        val_accuracy, val_loss = trainer.test(valloader)\n",
    "\n",
    "        logging.info(f\"Epoch: {epoch + 1}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.4f}, Time: {end - start:.4f}\")\n",
    "\n",
    "    accuracy, val_loss = trainer.test(testloader)\n",
    "    logging.info(f\"Test loss: {val_loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    setup_seed(42)\n",
    "    main(continue_train=False, testing=False, test_data_dir=\"./data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
